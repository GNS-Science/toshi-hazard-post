import importlib.resources as resources
from pathlib import Path

import numpy as np
import pyarrow.dataset as ds
from toshi_hazard_store.model.pyarrow import pyarrow_dataset

from toshi_hazard_post.aggregation import run_aggregation
from toshi_hazard_post.aggregation_args import load_input_args
import toshi_hazard_post.data

fixture_dir = resources.files('tests.fixtures.end_to_end')
args_filepath = fixture_dir / 'hazard.toml'

# this fixture was generated by toshi-hazard-post-v1
aggs_expected_filepath = fixture_dir / 'aggregations_275_PGA_WLG.npy'


def test_end_to_end(monkeypatch, tmp_path):
    probs_expected = np.load(aggs_expected_filepath)

    monkeypatch.setattr(toshi_hazard_post.data, 'AGG_DIR', str(tmp_path))
    monkeypatch.setattr(toshi_hazard_post.data, 'RLZ_DIR', str(Path(__file__).parent / 'fixtures/end_to_end/rlz'))

    # os.environ["THP_RLZ_DIR"] = str(Path(__file__).parent / 'fixtures/end_to_end/rlz')
    # os.environ["THP_AGG_DIR"] = str(tmp_path)
    # os.environ["THP_NUM_WORKERS"] = "1"

    agg_args = load_input_args(args_filepath)
    run_aggregation(agg_args)

    # read the aggregation back out and compare
    # rlz_dir, filesystem = pyarrow_dataset.configure_output(str(tmp_path))
    # dataset = ds.dataset(rlz_dir, format="parquet", filesystem=filesystem, partitioning="hive")
    # df = ds.Scanner.from_dataset(dataset).to_table().to_pandas()
    # print(df)
    probs = np.load(str(Path(tmp_path) / 'hazard.npy'))
    # probs = np.stack(ds.Scanner.from_dataset(dataset).to_table().to_pandas()['values'].values)

    np.testing.assert_allclose(probs, probs_expected, rtol=1e-07, atol=1e-08)
